{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qk2T8nam_hGY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGyx1uLDamPv",
        "outputId": "0d4a6730-a1ad-4a6a-c5dd-ddcdb4a94dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.17.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "TbrQ7Tthaz80",
        "outputId": "372d4ecb-1466-4ebe-e2e7-c10a4b8a4b2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_105222-6nyyfp5c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m013/test-1/runs/6nyyfp5c' target=\"_blank\">lilac-glitter-77</a></strong> to <a href='https://wandb.ai/cs22m013/test-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m013/test-1' target=\"_blank\">https://wandb.ai/cs22m013/test-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m013/test-1/runs/6nyyfp5c' target=\"_blank\">https://wandb.ai/cs22m013/test-1/runs/6nyyfp5c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m013/test-1/runs/6nyyfp5c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f760a136490>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(project=\"test-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COtCq4-FREDq"
      },
      "source": [
        "**Use the standard train/test split of fashion_mnist (use (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()). Keep 10% of the training data aside as validation data for this hyperparameter search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yznI2dZh_3lp"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X, y), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FPp3CkYsVnzV"
      },
      "source": [
        "# **Q1. Download the fashion-MNIST dataset and plot 1 sample image for each class.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "YS_HQgVWCXPA",
        "outputId": "b16afde5-fdb1-41be-9fdb-e664da96f890"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:6nyyfp5c) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lilac-glitter-77</strong> at: <a href='https://wandb.ai/cs22m013/test-1/runs/6nyyfp5c' target=\"_blank\">https://wandb.ai/cs22m013/test-1/runs/6nyyfp5c</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_105222-6nyyfp5c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:6nyyfp5c). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_105319-vqj583p7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m013/dl_assignment1/runs/vqj583p7' target=\"_blank\">classic-wind-201</a></strong> to <a href='https://wandb.ai/cs22m013/dl_assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m013/dl_assignment1' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m013/dl_assignment1/runs/vqj583p7' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1/runs/vqj583p7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k=len(class_names)\n",
        "images_plot,count=[],0\n",
        "for i in range(len(train_images)):\n",
        "  if(count>=k): break\n",
        "  if(count==train_labels[i]):\n",
        "    images_plot.append(train_images[i])\n",
        "    count+=1\n",
        "\n",
        "wandb.init(entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "wandb.log({\"Sample Image from each class\": [wandb.Image(image, caption=caption) for image, caption in zip(images_plot, class_names)]})\n",
        "wandb.run.name = \"Classes of Fashion_MNIST dataset\"\n",
        "wandb.run.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZKKdsr5V6p0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8YWiArGRUdv"
      },
      "source": [
        "**Flatten the Training, Testing & Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QCnM5WxeC1H9"
      },
      "outputs": [],
      "source": [
        "#flat the input\n",
        "image_size=28*28\n",
        "def flatten_input():\n",
        "  image_size=28*28\n",
        "  train_images_X=np.zeros(shape=(len(train_images),image_size),dtype='float32')  #60000x728\n",
        "  test_images_X=np.zeros(shape=(len(test_images),image_size),dtype='float32')    #10000x728\n",
        "  val_images_X=np.zeros(shape=(len(val_images),image_size),dtype='float32') \n",
        "\n",
        "  for i in range(len(train_images)):\n",
        "    train_images_X[i]=train_images[i].flatten()/255\n",
        "\n",
        "  for i in range(len(test_images)):\n",
        "    test_images_X[i]=test_images[i].flatten()/255\n",
        "\n",
        "  for i in range(len(val_images)):\n",
        "    val_images_X[i]=val_images[i].flatten()/255\n",
        "  return train_images_X,test_images_X,val_images_X\n",
        "\n",
        "train_images_X,test_images_X,val_images_X=flatten_input()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DPvCNJnR1nW"
      },
      "source": [
        "**Loss function for MSE && Cross Entroy with L2 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ibviwzepXejg"
      },
      "outputs": [],
      "source": [
        "def l2loss(parameters,lamda):\n",
        "  l2loss=0\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    l2loss+=(lamda/2)*np.sum(np.linalg.norm(parameters['W'+str(i)])**2)\n",
        "  return l2loss\n",
        "\n",
        "def loss_function(loss_type,true_output,predicted_output,parameters,lamda):\n",
        "  if(loss_type=='cross_entropy'):\n",
        "    return -1.0*np.sum(true_output*np.log(predicted_output+1e-9))\n",
        "  if(loss_type=='mse'):\n",
        "    return (1/2) * np.sum((true_output-predicted_output)**2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-lB2EOeR_E_"
      },
      "source": [
        "**Activation functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "anLyHw22E3Zv"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def sigmoid(x): \n",
        "   return 1./(1.+np.exp(-x))\n",
        "\n",
        "def derivative_relu(x):\n",
        "  return 1*(x>0) \n",
        "\n",
        "def softmax(x):\n",
        "  x=x-max(x)\n",
        "  return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "def identity(x):\n",
        "  return x\n",
        "\n",
        "def derivative_identity(x):\n",
        "  return np.ones(x.shape)\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  return sigmoid(x)*(np.ones_like(x)-sigmoid(x))\n",
        "\n",
        "def derivative_tanh(x):\n",
        "  return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def derivative_softmax(x):\n",
        "  return softmax(x) * (1-softmax(x))\n",
        "\n",
        "def activation_function(funct,x,derivative=False):\n",
        "  if derivative==True:\n",
        "    if funct=='softmax':\n",
        "      return derivative_softmax(x)\n",
        "    if funct==\"sigmoid\":\n",
        "      return derivative_sigmoid(x)\n",
        "    if funct==\"relu\":\n",
        "      return derivative_relu(x)\n",
        "    if funct==\"tanh\":\n",
        "      return derivative_tanh(x)\n",
        "    if funct==\"identity\":\n",
        "      return derivative_identity(x)\n",
        "  \n",
        "  else:\n",
        "    if funct=='softmax':\n",
        "      return softmax(x)\n",
        "    if funct==\"sigmoid\":\n",
        "      return sigmoid(x)\n",
        "    if funct==\"relu\":\n",
        "      return relu(x)\n",
        "    if funct==\"tanh\":\n",
        "      return tanh(x)\n",
        "    if funct==\"identity\":\n",
        "      return identity(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVUT0vQGSPqd"
      },
      "source": [
        "**Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx42XhHhOQuB"
      },
      "outputs": [],
      "source": [
        "\n",
        "#one hot encoding of labels\n",
        "def one_hot(i):\n",
        "  y=np.zeros((10,1))\n",
        "  y[i]=1\n",
        "  return y\n",
        "\n",
        "#returns predictions and accuracy for (images,labels).\n",
        "def find_pred(X,y,params,number_hidden_layers,hidden_layer_size,k,function):\n",
        "    y_pred=[]\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "        predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,params,hidden_layer_size,k,X[i],function)\n",
        "        y_pred.append(np.argmax(predicted_y))\n",
        "        if(np.argmax(predicted_y)==y[i]):\n",
        "          cnt+=1\n",
        "    accuracy=(cnt/len(X))*100\n",
        "    return y_pred,accuracy\n",
        "\n",
        "#returs Validation loss & Validation accuracy for Validation Set.\n",
        "def validationloss(X,y,params,lamda,number_hidden_layers,hidden_layer_size,function,loss_type):\n",
        "  cnt,loss=0,0\n",
        "  for i in range(len(X)):\n",
        "    predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,params,hidden_layer_size,k,X[i],function)\n",
        "    loss+=loss_function(loss_type,one_hot(y[i]),predicted_y,params,lamda)\n",
        "    if(np.argmax(predicted_y)==y[i]):\n",
        "      cnt+=1\n",
        "  loss+=l2loss(params,lamda)\n",
        "  Validation_loss=loss/len(X)  \n",
        "  Validation_accuracy=cnt/len(X)*100\n",
        "  return Validation_loss,Validation_accuracy\n",
        "\n",
        "#helper function for logging in WandB\n",
        "def calculate(loss,X,y,parameters,number_hidden_layers,hidden_layer_size,k,function,lamda,epoch,loss_type):\n",
        "  Training_loss=loss/len(X)\n",
        "  y_pred,Training_accuracy=find_pred(X,y,parameters,number_hidden_layers,hidden_layer_size,k,function)\n",
        "  Validation_loss,Validation_accuracy=validationloss(val_images_X,val_labels,parameters,lamda,number_hidden_layers,hidden_layer_size,function,loss_type)\n",
        "  print('Epoch:',epoch)\n",
        "  print('Training loss:',Training_loss)\n",
        "  print('Training_accuracy',Training_accuracy)\n",
        "  print('Validation loss:', Validation_loss)\n",
        "  print('Validation accuracy:',Validation_accuracy)\n",
        "  wandb.log({'Training_accuracy':Training_accuracy,'Epoch':epoch,'Training_loss':Training_loss,'Validation_loss':Validation_loss,'Validation_accuracy':Validation_accuracy})\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drF9cF_pS3cA"
      },
      "source": [
        "**Helper functions for initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Sb3aVZrFnltj"
      },
      "outputs": [],
      "source": [
        "def init(n_in, n_out,initialization):\n",
        "    if(initialization=='random'):\n",
        "      return np.random.default_rng().uniform(low=-0.69,high=0.69,size=(n_in,n_out))\n",
        "    if(initialization=='xavier'):\n",
        "        return  np.random.randn(n_in,n_out)*np.sqrt(2/(n_in+n_out))\n",
        "    if(initialization=='zero'):\n",
        "      return np.zeros((n_in,n_out))\n",
        "\n",
        "#initializes parameters w.r.t to number of layers & initialization provided.\n",
        "def initialize_parameters(number_of_neurons,number_hidden_layers,k,layers,initialization):\n",
        "  parameters={}\n",
        "  parameters['W'+str(1)]=init(layers[0],image_size,initialization)\n",
        "  parameters['b'+str(1)]=init(layers[0],1,initialization)\n",
        "  for i in range(1,number_hidden_layers):\n",
        "    parameters['W'+str(i+1)]=init(layers[i],layers[i-1],initialization)\n",
        "    parameters['b'+str(i+1)]=init(layers[i],1,initialization)\n",
        "  parameters['W'+str(number_hidden_layers+1)]=init(k,layers[-1],initialization)\n",
        "  parameters['b'+str(number_hidden_layers+1)]=init(k,1,initialization)\n",
        "  return parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eGcfp0b6KeXh"
      },
      "source": [
        "# **Q2.Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OD0le4R0Rxad"
      },
      "outputs": [],
      "source": [
        "def feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,data,function):\n",
        "  activation={}\n",
        "  pre_activation={}\n",
        "  activation['h0']=data.reshape(784,1)\n",
        "  for i in range(1,number_hidden_layers+1):\n",
        "    a=np.add(parameters['b'+str(i)],np.matmul(parameters['W'+str(i)],activation['h'+str(i-1)]))\n",
        "    h=activation_function(function,a)\n",
        "    #h=sigmoid(a)\n",
        "    pre_activation['a'+str(i)]=a\n",
        "    activation['h'+str(i)]=h\n",
        "  \n",
        "  a=np.add(parameters['b'+str(number_hidden_layers+1)],np.matmul(parameters['W'+str(number_hidden_layers+1)],activation['h'+str(number_hidden_layers)]))\n",
        "  h=softmax(a)\n",
        "  pre_activation['a'+str(number_hidden_layers+1)]=a\n",
        "  activation['h'+str(number_hidden_layers+1)]=h\n",
        "  \n",
        "  return h,activation,pre_activation\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fGdobG7QK7iO"
      },
      "source": [
        "# **Q3.Implement the backpropagation algorithm with support for the following optimisation functions**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gcYvTmD09HAY"
      },
      "outputs": [],
      "source": [
        "#h:: activation a=:preactivation\n",
        "def back_propogation(parameters,activation,pre_activation,X,y,number_hidden_layers,predicted_y,k,lamda,function,loss_type):\n",
        "\n",
        "  gradient_parameters,gradient_activation,gradient_preactivation={},{},{}\n",
        "\n",
        "  #compute output gradient\n",
        "  e_y=np.zeros((k,1))\n",
        "  e_y[y][0]=1\n",
        "  if(loss_type=='cross_entropy'):\n",
        "    gradient_preactivation['a'+str(number_hidden_layers+1)]=-(e_y-predicted_y)\n",
        "  else:\n",
        "    gradient_preactivation['a'+str(number_hidden_layers+1)]=(predicted_y-e_y)*activation_function('softmax',pre_activation['a'+str(number_hidden_layers+1)],True)\n",
        "\n",
        "  for t in range(number_hidden_layers+1,0,-1):\n",
        "    #compute gradients w.r.t parameters\n",
        "    gradient_parameters['W'+str(t)]=np.matmul(gradient_preactivation['a'+str(t)],activation['h'+str(t-1)].T) \n",
        "    gradient_parameters['b'+str(t)]=gradient_preactivation['a'+str(t)]\n",
        "    #print(t,gradient_parameters['W'+str(t)])\n",
        "    if(t==1):break\n",
        "    #compute gradients w.r.t layers below\n",
        "    gradient_activation['h'+str(t-1)]=np.matmul(parameters['W'+str(t)].T,gradient_preactivation['a'+str(t)])\n",
        "\n",
        "    #compute gradients w.r.t preactivation layer\n",
        "    gradient_preactivation['a'+str(t-1)]=np.multiply(gradient_activation['h'+str(t-1)],activation_function(function,pre_activation['a'+str(t-1)],True))\n",
        "  #print(pre_activation)\n",
        "  return gradient_parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xa7SBRnrTT6J"
      },
      "source": [
        "### Helper functions for update Rules in optimization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JiUENjXlKNZN"
      },
      "outputs": [],
      "source": [
        "#For SGD\n",
        "def update_parameters(parameters,gradient_change,learning_rate):\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    parameters['W'+str(i)]=parameters['W'+str(i)]-learning_rate*gradient_change['W'+str(i)]\n",
        "    parameters['b'+str(i)]=parameters['b'+str(i)]-learning_rate*gradient_change['b'+str(i)]\n",
        "  return parameters\n",
        "\n",
        "#For Momentum\n",
        "def update_parameters_momentum(parameters,gradient_change,prior_updates,learning_rate,beta):\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    prior_updates['W'+str(i)]=beta*prior_updates['W'+str(i)]+gradient_change['W'+str(i)]\n",
        "    parameters['W'+str(i)]=parameters['W'+str(i)]-learning_rate*prior_updates['W'+str(i)]\n",
        "\n",
        "    prior_updates['b'+str(i)]=beta*prior_updates['b'+str(i)]+gradient_change['b'+str(i)]\n",
        "    parameters['b'+str(i)]=parameters['b'+str(i)]-learning_rate*prior_updates['b'+str(i)]\n",
        "  return parameters,prior_updates\n",
        "\n",
        "#For RMSprop\n",
        "def update_parameters_rmsprop(parameters,gradient_change,prior_updates,learning_rate,beta):\n",
        "  epsilon=1e-9\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    prior_updates['W'+str(i)]=beta*prior_updates['W'+str(i)]+(1-beta)*(gradient_change['W'+str(i)])**2\n",
        "    parameters['W'+str(i)]=parameters['W'+str(i)]-gradient_change['W'+str(i)]*(learning_rate/np.sqrt(prior_updates['W'+str(i)]+epsilon))\n",
        "\n",
        "    prior_updates['b'+str(i)]=beta*prior_updates['b'+str(i)]+(1-beta)*(gradient_change['b'+str(i)])**2\n",
        "    parameters['b'+str(i)]=parameters['b'+str(i)]-gradient_change['b'+str(i)]*(learning_rate/np.sqrt(prior_updates['b'+str(i)]+epsilon))\n",
        "  return parameters,prior_updates\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NZqG1RiWLcV9"
      },
      "source": [
        "## **Nesterov Accelerated Gradient Descent**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DhCxKpquqDHd"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_nag(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type):\n",
        "  #initializing parameters,prior_updates,updates \n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  prior_updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  temp=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  for epoch in range(max_epochs):\n",
        "    loss,cnt=0,0\n",
        "    for it in range(1,len(parameters)//2+1):\n",
        "      updates['W'+str(it)]=prior_updates['W'+str(it)]*beta\n",
        "      updates['b'+str(it)]=prior_updates['b'+str(it)]*beta\n",
        "    \n",
        "    for it in range(1,len(parameters)//2+1):\n",
        "      temp['W'+str(it)]=parameters['W'+str(it)]-updates['W'+str(it)]\n",
        "      temp['b'+str(it)]=parameters['b'+str(it)]-updates['b'+str(it)]\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,temp,hidden_layer_size,k,X[i],function)\n",
        "      loss+=loss_function(loss_type,one_hot(y[i]),predicted_y,parameters,lamda)\n",
        "      gradient_parameters=back_propogation(temp,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function,loss_type)\n",
        "      \n",
        "      if(cnt==0):\n",
        "        gradient_change=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "\n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "\n",
        "        #update rule\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          updates['W'+str(it)]=beta*prior_updates['W'+str(it)]+learning_rate*gradient_change['W'+str(it)]\n",
        "          parameters['W'+str(it)]=parameters['W'+str(it)]-updates['W'+str(it)]\n",
        "          temp['W'+str(it)]=parameters['W'+str(it)]\n",
        "          prior_updates['W'+str(it)]=updates['W'+str(it)]\n",
        "\n",
        "          updates['b'+str(it)]=beta*prior_updates['b'+str(it)]+learning_rate*gradient_change['b'+str(it)]\n",
        "          parameters['b'+str(it)]=parameters['b'+str(it)]-updates['b'+str(it)]\n",
        "          temp['b'+str(it)]=parameters['b'+str(it)]\n",
        "          prior_updates['b'+str(it)]=updates['b'+str(it)]\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    \n",
        "    loss+=l2regularizedloss\n",
        "    calculate(loss,X,y,parameters,number_hidden_layers,hidden_layer_size,k,function,lamda,epoch,loss_type)\n",
        "\n",
        "  return parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1AYZAkqPLovd"
      },
      "source": [
        "## **Stochastic Gradient Descent**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UEuMQiwAjOUw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent_sgd(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type):\n",
        "  #initializing parameters\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  \n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      loss+=loss_function(loss_type,one_hot(y[i]),predicted_y,parameters,lamda)\n",
        "\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function,loss_type)\n",
        "    \n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "\n",
        "        if(lamda!=0):\n",
        "           for it in range(1,len(parameters)//2+1):\n",
        "             gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "        parameters=update_parameters(parameters,gradient_change,learning_rate)\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    l2regularizedloss/=len(X)\n",
        "    calculate(loss,X,y,parameters,number_hidden_layers,hidden_layer_size,k,function,lamda,epoch,loss_type)\n",
        "\n",
        "\n",
        "  return parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I4jN0_VILvWI"
      },
      "source": [
        "## **Momentum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fh348aaImAeB"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_momentum(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type):\n",
        "  #initializing parameters\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      #print(activation['h4'])\n",
        "      \n",
        "      loss+=loss_function(loss_type,one_hot(y[i]),predicted_y,parameters,lamda)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function,loss_type)\n",
        "      if( epoch==0 and i==0):\n",
        "        #initialize with zero\n",
        "        prior_updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "\n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "\n",
        "        \n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "        parameters,prior_updates=update_parameters_momentum(parameters,gradient_change,prior_updates,learning_rate,beta)\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "\n",
        "    calculate(loss,X,y,parameters,number_hidden_layers,hidden_layer_size,k,function,lamda,epoch,loss_type)\n",
        "\n",
        "  return parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vhrbUPlsLyz_"
      },
      "source": [
        "# **RMSprop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0RJThIwHzaNu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent_rmsprop(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type):\n",
        "  #initializing parameters\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      #print(activation['h4'])\n",
        "      \n",
        "      loss+=loss_function(loss_type,one_hot(y[i]),predicted_y,parameters,lamda)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function,loss_type)\n",
        "      if( epoch==0 and i==0):\n",
        "        #initialize with zero\n",
        "        prior_updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "\n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "\n",
        "        \n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "        parameters,prior_updates=update_parameters_rmsprop(parameters,gradient_change,prior_updates,learning_rate,beta)\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "    \n",
        "    calculate(loss,X,y,parameters,number_hidden_layers,hidden_layer_size,k,function,lamda,epoch,loss_type)\n",
        "\n",
        "  return parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n-cObAS1L3e7"
      },
      "source": [
        "## **Adam & Nadam**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TFM7Jy9wWckQ"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type,beta1,beta2):\n",
        "  #initializing parameters,momentum_hat,update_hat\n",
        "\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  momentum=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  momentum_hat=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  update=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  update_hat=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    epsilon=1e-10\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      loss+=loss_function(loss_type,one_hot(y[i]),predicted_y,parameters,lamda)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function,loss_type)\n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "\n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        \n",
        "        cnt=0\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          momentum['W'+str(it)]=beta1*momentum['W'+str(it)]+(1-beta1)*gradient_change['W'+str(it)]\n",
        "          momentum['b'+str(it)]=beta1*momentum['b'+str(it)]+(1-beta1)*gradient_change['b'+str(it)]\n",
        "          momentum_hat['W'+str(it)]=momentum['W'+str(it)]/(1-beta1**(epoch+1))\n",
        "          momentum_hat['b'+str(it)]=momentum['b'+str(it)]/(1-beta1**(epoch+1))\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          update['W'+str(it)]=beta2*update['W'+str(it)]+(1-beta2)*gradient_change['W'+str(it)]**2\n",
        "          update['b'+str(it)]=beta2*update['b'+str(it)]+(1-beta2)*gradient_change['b'+str(it)]**2\n",
        "          update_hat['W'+str(it)]=update['W'+str(it)]/(1-beta2**(epoch+1))\n",
        "          update_hat['b'+str(it)]=update['b'+str(it)]/(1-beta2**(epoch+1))\n",
        "\n",
        "        if(optimizer=='adam'):\n",
        "          #update rule for adam\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            parameters['W'+str(it)]=parameters['W'+str(it)]-(learning_rate*momentum_hat['W'+str(it)]/np.sqrt(update_hat['W'+str(it)]+epsilon))\n",
        "            parameters['b'+str(it)]=parameters['b'+str(it)]-(learning_rate*momentum_hat['b'+str(it)]/np.sqrt(update_hat['b'+str(it)]+epsilon))\n",
        "        else:\n",
        "          #update rule for nadam  \n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            parameters['W'+str(it)]=parameters['W'+str(it)]-(learning_rate/np.sqrt(update_hat['W'+str(it)]+epsilon))*(beta1*momentum_hat['W'+str(it)]+(1-beta1)*gradient_change['W'+str(it)]/(1-beta1**(epoch+1)))\n",
        "            parameters['b'+str(it)]=parameters['b'+str(it)]-(learning_rate/np.sqrt(update_hat['b'+str(it)]+epsilon))*(beta1*momentum_hat['b'+str(it)]+(1-beta1)*gradient_change['b'+str(it)]/(1-beta1**(epoch+1)))\n",
        "    \n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    \n",
        "    loss+=l2regularizedloss\n",
        "\n",
        "    calculate(loss,X,y,parameters,number_hidden_layers,hidden_layer_size,k,function,lamda,epoch,loss_type)\n",
        "\n",
        "  return parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PknhHgrjNLX6"
      },
      "source": [
        "**Neural Network function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7VoHBLyXc9SX"
      },
      "outputs": [],
      "source": [
        "def NeuralNetwork():\n",
        "  k=10\n",
        "  beta=0.9 \n",
        "  beta1=0.9\n",
        "  beta2=0.99\n",
        "  wandb.init()\n",
        "  config=wandb.config\n",
        "  loss_type=config.loss_type\n",
        "  number_hidden_layers=config.number_hidden_layers\n",
        "  hidden_layer_size=config.hidden_layer_size\n",
        "  batch_size=config.batch_size\n",
        "  max_epochs=config.max_epochs\n",
        "  optimizer=config.optimizer\n",
        "  function=config.function\n",
        "  learning_rate=config.learning_rate\n",
        "  lamda=config.lamda\n",
        "  initialization=config.initialization\n",
        "  layers=[hidden_layer_size for i in range(number_hidden_layers)]\n",
        "  run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}_loss_{}\".format(learning_rate, function,initialization, optimizer, batch_size, lamda, max_epochs, hidden_layer_size, number_hidden_layers,loss_type)\n",
        "  print(run_name)\n",
        "  #can add optimizer if needed\n",
        "  if(optimizer=='sgd'):\n",
        "    params=gradient_descent_sgd(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type)\n",
        "  if(optimizer=='nag'):\n",
        "    params=gradient_descent_nag(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type)\n",
        "  if(optimizer=='momentum'):\n",
        "    params=gradient_descent_momentum(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type)\n",
        "  if(optimizer=='rmsprop'):\n",
        "    params=gradient_descent_rmsprop(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type)\n",
        "  if(optimizer=='adam'):\n",
        "    params=gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type,beta1,beta2)\n",
        "  if(optimizer=='nadam'):\n",
        "    params=gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type,beta1,beta2)\n",
        "  \n",
        "  pred_labels,accuracy=find_pred(test_images_X,test_labels,params,number_hidden_layers,hidden_layer_size,k,function)\n",
        "  print()\n",
        "  print(\"Testing Accuracy:\",accuracy)\n",
        "  wandb.run.name = run_name\n",
        "  wandb.run.save()\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xqjPs8GwNZMz"
      },
      "source": [
        "# **Q4.Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IvzHE3e1B_t"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment - MSE & Cross Entropy Error Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"Validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"bayes\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001,0.1]\n",
        "        },\n",
        "        \"function\": {\n",
        "            \"values\": [\"relu\",\"tanh\",\"sigmoid\"]\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"values\": [\"xavier\",'random']\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"adam\",\"sgd\",\"momentum\",\"nadam\",\"rmsprop\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32,16,64]\n",
        "        },\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [10,5]\n",
        "        },\n",
        "        \"lamda\": {\n",
        "            \"values\": [0.0005,0.5,0]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [64,32,128]\n",
        "        },\n",
        "        \"number_hidden_layers\": {\n",
        "            \"values\": [3,4,5]\n",
        "        },\n",
        "        \"loss_type\":{\n",
        "            \"values\":['cross_entropy','mse']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "wandb.agent(sweep_id, NeuralNetwork, count=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6hhLbTPPd-HJ"
      },
      "source": [
        "## **Report the accuracy on the test set of fashion_mnist and plot the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SffGBB1ZbzV3",
        "outputId": "a3568b68-dbfa-4aa4-9bd2-b45c3ffb8bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.001_ac_relu_in_xavier_op_adam_bs_32_L2_0.0005_ep_10_nn_64_nh_5_loss_cross_entropy\n",
            "Epoch: 0\n",
            "Training loss: 0.5355055954120173\n",
            "Training_accuracy 84.88888888888889\n",
            "Validation loss: 0.4212272673147908\n",
            "Validation accuracy: 84.6\n",
            "Epoch: 1\n",
            "Training loss: 0.38300197462476043\n",
            "Training_accuracy 86.74444444444444\n",
            "Validation loss: 0.38597467742762265\n",
            "Validation accuracy: 86.1\n",
            "Epoch: 2\n",
            "Training loss: 0.34584950696958056\n",
            "Training_accuracy 87.53703703703704\n",
            "Validation loss: 0.37563364259971965\n",
            "Validation accuracy: 86.63333333333333\n",
            "Epoch: 3\n",
            "Training loss: 0.3214613277012385\n",
            "Training_accuracy 88.71666666666667\n",
            "Validation loss: 0.359741932703385\n",
            "Validation accuracy: 87.03333333333333\n",
            "Epoch: 4\n",
            "Training loss: 0.3043099202291101\n",
            "Training_accuracy 89.22962962962963\n",
            "Validation loss: 0.3522526804262778\n",
            "Validation accuracy: 87.36666666666667\n",
            "Epoch: 5\n",
            "Training loss: 0.2906017986249653\n",
            "Training_accuracy 89.65925925925926\n",
            "Validation loss: 0.3479851840455489\n",
            "Validation accuracy: 87.86666666666667\n",
            "Epoch: 6\n",
            "Training loss: 0.2788280604932584\n",
            "Training_accuracy 89.77222222222223\n",
            "Validation loss: 0.3525996476965521\n",
            "Validation accuracy: 87.93333333333334\n",
            "Epoch: 7\n",
            "Training loss: 0.26954487953300044\n",
            "Training_accuracy 90.28888888888889\n",
            "Validation loss: 0.34720922943169474\n",
            "Validation accuracy: 88.25\n",
            "Epoch: 8\n",
            "Training loss: 0.2596210497101842\n",
            "Training_accuracy 90.37407407407407\n",
            "Validation loss: 0.3555426538250548\n",
            "Validation accuracy: 88.11666666666666\n",
            "Epoch: 9\n",
            "Training loss: 0.25120351596827045\n",
            "Training_accuracy 90.58148148148149\n",
            "Validation loss: 0.36105796926384165\n",
            "Validation accuracy: 88.18333333333334\n",
            "\n",
            "Testing Accuracy: 87.6\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:vqj583p7) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td></td></tr><tr><td>Training_accuracy</td><td></td></tr><tr><td>Training_loss</td><td></td></tr><tr><td>Validation_accuracy</td><td></td></tr><tr><td>Validation_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>9</td></tr><tr><td>Training_accuracy</td><td>90.58148</td></tr><tr><td>Training_loss</td><td>0.2512</td></tr><tr><td>Validation_accuracy</td><td>88.18333</td></tr><tr><td>Validation_loss</td><td>0.36106</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">classic-wind-201</strong> at: <a href='https://wandb.ai/cs22m013/dl_assignment1/runs/vqj583p7' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1/runs/vqj583p7</a><br/>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_105319-vqj583p7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:vqj583p7). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_110854-42uf00gr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m013/dl_assignment1/runs/42uf00gr' target=\"_blank\">lunar-forest-202</a></strong> to <a href='https://wandb.ai/cs22m013/dl_assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m013/dl_assignment1' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m013/dl_assignment1/runs/42uf00gr' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1/runs/42uf00gr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Best Configuration\n",
        "k=10\n",
        "beta=0.9 \n",
        "beta1=0.9\n",
        "beta2=0.99\n",
        "loss_type=\"cross_entropy\"\n",
        "number_hidden_layers=5\n",
        "hidden_layer_size=64\n",
        "batch_size=32\n",
        "max_epochs=10\n",
        "optimizer=\"adam\"\n",
        "function=\"relu\"\n",
        "learning_rate=0.001\n",
        "lamda=0.0005\n",
        "initialization=\"xavier\"\n",
        "layers=[hidden_layer_size for i in range(number_hidden_layers)]\n",
        "run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}_loss_{}\".format(learning_rate, function,initialization, optimizer, batch_size, lamda, max_epochs, hidden_layer_size, number_hidden_layers,loss_type)\n",
        "print(run_name)\n",
        "params=gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type,beta1,beta2)\n",
        "  \n",
        "pred_labels,accuracy=find_pred(test_images_X,test_labels,params,number_hidden_layers,hidden_layer_size,k,function)\n",
        "print()\n",
        "print(\"Testing Accuracy:\",accuracy)\n",
        "wandb.init(entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "wandb.log({\"confusion_matrix\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                        y_true=test_labels, preds=np.array(pred_labels),\n",
        "                        class_names=class_names)})\n",
        "\n",
        "wandb.run.name=\"Confusion Matrix\"\n",
        "wandb.run.save()\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hd5PfbKLeS79"
      },
      "source": [
        "\n",
        "# **Q8.In all the models above you would have used cross entropy loss. Now compare the cross entropy loss with the squared error loss.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F76zGsMeTb_"
      },
      "outputs": [],
      "source": [
        "#Best confiig 1\n",
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment - MSE & Cross Entropy Error Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"Validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001]\n",
        "        },\n",
        "        \"function\": {\n",
        "            \"values\": [\"relu\"]\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"values\": [\"xavier\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"adam\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32]\n",
        "        },\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [10]\n",
        "        },\n",
        "        \"lamda\": {\n",
        "            \"values\": [0.0005]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [64]\n",
        "        },\n",
        "        \"number_hidden_layers\": {\n",
        "            \"values\": [5]\n",
        "        },\n",
        "        \"loss_type\":{\n",
        "            \"values\":['cross_entropy','mse']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "wandb.agent(sweep_id, NeuralNetwork, count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-7eS68l6kH"
      },
      "outputs": [],
      "source": [
        "#Best confiig 2\n",
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment - MSE & Cross Entropy Error Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"Validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.1]\n",
        "        },\n",
        "        \"function\": {\n",
        "            \"values\": [\"relu\"]\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"values\": [\"xavier\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [64]\n",
        "        },\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [10]\n",
        "        },\n",
        "        \"lamda\": {\n",
        "            \"values\": [0]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [64]\n",
        "        },\n",
        "        \"number_hidden_layers\": {\n",
        "            \"values\": [3]\n",
        "        },\n",
        "        \"loss_type\":{\n",
        "            \"values\":['cross_entropy','mse']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "wandb.agent(sweep_id, NeuralNetwork, count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fepVODofmFU8"
      },
      "outputs": [],
      "source": [
        "#Best config 3\n",
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment - MSE & Cross Entropy Error Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"Validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001]\n",
        "        },\n",
        "        \"function\": {\n",
        "            \"values\": [\"tanh\"]\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"values\": [\"xavier\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"nadam\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16]\n",
        "        },\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [10]\n",
        "        },\n",
        "        \"lamda\": {\n",
        "            \"values\": [0]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [32]\n",
        "        },\n",
        "        \"number_hidden_layers\": {\n",
        "            \"values\": [4]\n",
        "        },\n",
        "        \"loss_type\":{\n",
        "            \"values\":['cross_entropy','mse']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "wandb.agent(sweep_id, NeuralNetwork, count=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-XtQTO2Amnwj"
      },
      "source": [
        "# **Q10. Give me 3 recommendations for what would work for the MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TvrxWHkEob1D"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.mnist\n",
        "(X, y), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "train_images_X,test_images_X,val_images_X=flatten_input()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C3vlHPIoxtX"
      },
      "outputs": [],
      "source": [
        "#Best Config1\n",
        "wandb.init(entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "k=10\n",
        "beta=0.9 \n",
        "beta1=0.9\n",
        "beta2=0.99\n",
        "loss_type=\"cross_entropy\"\n",
        "number_hidden_layers=5\n",
        "hidden_layer_size=64\n",
        "batch_size=32\n",
        "max_epochs=10\n",
        "optimizer=\"adam\"\n",
        "function=\"relu\"\n",
        "learning_rate=0.001\n",
        "lamda=0.0005\n",
        "initialization=\"xavier\"\n",
        "layers=[hidden_layer_size for i in range(number_hidden_layers)]\n",
        "run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}_loss_{}\".format(learning_rate, function,initialization, optimizer, batch_size, lamda, max_epochs, hidden_layer_size, number_hidden_layers,loss_type)\n",
        "print(run_name)\n",
        "params=gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type,beta1,beta2)\n",
        "pred_labels,accuracy=find_pred(test_images_X,test_labels,params,number_hidden_layers,hidden_layer_size,k,function)\n",
        "print()\n",
        "print(\"Testing Accuracy:\",accuracy)\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3cd6fd6f39bc414094f3885f8d05f53c",
            "93369d40891f4984b8c3b6707827b830",
            "4fad238a23db49d5974dc0a7537cb92e",
            "b1f8b4292b0445f8a4449655a775675c",
            "263b73e3e0f940449645c4adc638a908",
            "634583c3f1ff411f834b9ab3c7a6456b",
            "c173d83a0ece4118af5e807873621171",
            "40a7b66858944c3d94f15fb6aad4aec4"
          ]
        },
        "id": "9sCqQ7S0pMGY",
        "outputId": "146fa80a-bea0-45d2-d908-7ddd32c14d7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:pogrkuvj) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cd6fd6f39bc414094f3885f8d05f53c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dark-terrain-206</strong> at: <a href='https://wandb.ai/cs22m013/dl_assignment1/runs/pogrkuvj' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1/runs/pogrkuvj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_120144-pogrkuvj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:pogrkuvj). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_120200-t5vwscbq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m013/dl_assignment1/runs/t5vwscbq' target=\"_blank\">vital-wind-207</a></strong> to <a href='https://wandb.ai/cs22m013/dl_assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m013/dl_assignment1' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m013/dl_assignment1/runs/t5vwscbq' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment1/runs/t5vwscbq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.1_ac_relu_in_xavier_op_sgd_bs_64_L2_0_ep_10_nn_64_nh_3_loss_cross_entropy\n",
            "Epoch: 0\n",
            "Training loss: 0.4002218551443951\n",
            "Training_accuracy 93.93518518518519\n",
            "Validation loss: 0.20474723279489493\n",
            "Validation accuracy: 93.89999999999999\n",
            "Epoch: 1\n",
            "Training loss: 0.17535337884077473\n",
            "Training_accuracy 95.9462962962963\n",
            "Validation loss: 0.14142716497278382\n",
            "Validation accuracy: 95.65\n",
            "Epoch: 2\n",
            "Training loss: 0.12685592194475184\n",
            "Training_accuracy 96.94259259259259\n",
            "Validation loss: 0.12032333544533355\n",
            "Validation accuracy: 96.5\n",
            "Epoch: 3\n",
            "Training loss: 0.10011242759890493\n",
            "Training_accuracy 97.42222222222222\n",
            "Validation loss: 0.11173637462057351\n",
            "Validation accuracy: 96.68333333333334\n",
            "Epoch: 4\n",
            "Training loss: 0.08155278336955364\n",
            "Training_accuracy 97.8462962962963\n",
            "Validation loss: 0.10355311807900783\n",
            "Validation accuracy: 96.98333333333333\n",
            "Epoch: 5\n",
            "Training loss: 0.06857019743882108\n",
            "Training_accuracy 98.0537037037037\n",
            "Validation loss: 0.099737558929199\n",
            "Validation accuracy: 97.13333333333334\n",
            "Epoch: 6\n",
            "Training loss: 0.058838021152085335\n",
            "Training_accuracy 98.24259259259259\n",
            "Validation loss: 0.1001589413849303\n",
            "Validation accuracy: 97.2\n",
            "Epoch: 7\n",
            "Training loss: 0.0498104661158505\n",
            "Training_accuracy 98.42592592592592\n",
            "Validation loss: 0.10105194191345386\n",
            "Validation accuracy: 96.91666666666666\n",
            "Epoch: 8\n",
            "Training loss: 0.04197903638270546\n",
            "Training_accuracy 98.6037037037037\n",
            "Validation loss: 0.09893646371931429\n",
            "Validation accuracy: 97.15\n",
            "Epoch: 9\n",
            "Training loss: 0.035782138640375404\n",
            "Training_accuracy 98.44074074074074\n",
            "Validation loss: 0.1116213970304049\n",
            "Validation accuracy: 96.85000000000001\n",
            "\n",
            "Testing Accuracy: 96.87\n"
          ]
        }
      ],
      "source": [
        "#Best Config2\n",
        "wandb.init(entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "k=10\n",
        "beta=0.9 \n",
        "beta1=0.9\n",
        "beta2=0.99\n",
        "loss_type=\"cross_entropy\"\n",
        "number_hidden_layers=3\n",
        "hidden_layer_size=64\n",
        "batch_size=64\n",
        "max_epochs=10\n",
        "optimizer=\"sgd\"\n",
        "function=\"relu\"\n",
        "learning_rate=0.1\n",
        "lamda=0\n",
        "initialization=\"xavier\"\n",
        "layers=[hidden_layer_size for i in range(number_hidden_layers)]\n",
        "run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}_loss_{}\".format(learning_rate, function,initialization, optimizer, batch_size, lamda, max_epochs, hidden_layer_size, number_hidden_layers,loss_type)\n",
        "print(run_name)\n",
        "params=gradient_descent_sgd(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type)\n",
        "pred_labels,accuracy=find_pred(test_images_X,test_labels,params,number_hidden_layers,hidden_layer_size,k,function)\n",
        "print()\n",
        "print(\"Testing Accuracy:\",accuracy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F85j8DuLpkRU"
      },
      "outputs": [],
      "source": [
        "#Best Config3\n",
        "wandb.init(entity=\"cs22m013\", project=\"dl_assignment1\")\n",
        "k=10\n",
        "beta=0.9 \n",
        "beta1=0.9\n",
        "beta2=0.99\n",
        "loss_type=\"cross_entropy\"\n",
        "number_hidden_layers=5\n",
        "hidden_layer_size=64\n",
        "batch_size=32\n",
        "max_epochs=10\n",
        "optimizer=\"nadam\"\n",
        "function=\"tanh\"\n",
        "learning_rate=0.001\n",
        "lamda=0\n",
        "initialization=\"xavier\"\n",
        "layers=[hidden_layer_size for i in range(number_hidden_layers)]\n",
        "run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}_loss_{}\".format(learning_rate, function,initialization, optimizer, batch_size, lamda, max_epochs, hidden_layer_size, number_hidden_layers,loss_type)\n",
        "print(run_name)\n",
        "params=gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function,loss_type,beta1,beta2)\n",
        "pred_labels,accuracy=find_pred(test_images_X,test_labels,params,number_hidden_layers,hidden_layer_size,k,function)\n",
        "print()\n",
        "print(\"Testing Accuracy:\",accuracy)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "263b73e3e0f940449645c4adc638a908": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cd6fd6f39bc414094f3885f8d05f53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93369d40891f4984b8c3b6707827b830",
              "IPY_MODEL_4fad238a23db49d5974dc0a7537cb92e"
            ],
            "layout": "IPY_MODEL_b1f8b4292b0445f8a4449655a775675c"
          }
        },
        "40a7b66858944c3d94f15fb6aad4aec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fad238a23db49d5974dc0a7537cb92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c173d83a0ece4118af5e807873621171",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40a7b66858944c3d94f15fb6aad4aec4",
            "value": 1
          }
        },
        "634583c3f1ff411f834b9ab3c7a6456b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93369d40891f4984b8c3b6707827b830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_263b73e3e0f940449645c4adc638a908",
            "placeholder": "",
            "style": "IPY_MODEL_634583c3f1ff411f834b9ab3c7a6456b",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "b1f8b4292b0445f8a4449655a775675c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c173d83a0ece4118af5e807873621171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
