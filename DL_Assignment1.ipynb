{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qk2T8nam_hGY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yznI2dZh_3lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ab78a5-a370-4569-89d2-1be7a4dc002a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "#(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "(X, y), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPp3CkYsVnzV"
      },
      "source": [
        "Download the fashion-MNIST dataset and plot 1 sample image for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "YS_HQgVWCXPA",
        "outputId": "a6099eab-2107-4c33-97aa-ff6989b19bef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k=len(class_names)\n",
        "images_plot,count=[],0\n",
        "for i in range(len(train_images)):\n",
        "  if(count>=k): break\n",
        "  if(count==train_labels[i]):\n",
        "    images_plot.append(train_images[i])\n",
        "    count+=1\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(10):\n",
        "     plt.subplot(5,5,i+1)\n",
        "     plt.xticks([])\n",
        "     plt.yticks([])\n",
        "     plt.imshow(images_plot[i], cmap=plt.cm.binary)\n",
        "     plt.xlabel(class_names[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZKKdsr5V6p0"
      },
      "source": [
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QCnM5WxeC1H9"
      },
      "outputs": [],
      "source": [
        "#flat the input\n",
        "image_size=28*28\n",
        "train_images_X=np.zeros(shape=(len(train_images),image_size),dtype='float32')  #60000x728\n",
        "test_images_X=np.zeros(shape=(len(test_images),image_size),dtype='float32')    #10000x728\n",
        "val_images_X=np.zeros(shape=(len(val_images),image_size),dtype='float32') \n",
        "\n",
        "for i in range(len(train_images)):\n",
        "  train_images_X[i]=train_images[i].flatten()/255\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "  test_images_X[i]=test_images[i].flatten()/255\n",
        "\n",
        "for i in range(len(val_images)):\n",
        "  val_images_X[i]=val_images[i].flatten()/255\n",
        "\n",
        "def one_hot(i):\n",
        "  y=np.zeros((10,1))\n",
        "  y[i]=1\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3ohmsqeLCPWk"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def sigmoid(x): \n",
        "   return 1./(1.+np.exp(-x))\n",
        "\n",
        "def derivative_relu(x):\n",
        "  return 1*(x>0) \n",
        "\n",
        "def softmax(x):\n",
        "  x=x-max(x)\n",
        "  return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  return sigmoid(x)*(np.ones_like(x)-sigmoid(x))\n",
        "\n",
        "def derivative_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "\n",
        "def cross_entropy(true_output,predicted_output):\n",
        "     return -1.0*np.sum(true_output*np.log(predicted_output+1e-9))\n",
        "  \n",
        "def l2loss(parameters,lamda):\n",
        "  l2loss=0\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    l2loss+=(lamda/2)*np.sum(np.linalg.norm(parameters['W'+str(i)])**2)\n",
        "  return l2loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "anLyHw22E3Zv"
      },
      "outputs": [],
      "source": [
        "def activation_function(funct,x,derivative=False):\n",
        "  if derivative==True:\n",
        "    if funct==\"sigmoid\":\n",
        "      return derivative_sigmoid(x)\n",
        "    if funct==\"relu\":\n",
        "      return derivative_relu(x)\n",
        "    if funct==\"tanh\":\n",
        "      return derivative_tanh(x)\n",
        "  \n",
        "  else:\n",
        "    if funct==\"sigmoid\":\n",
        "      return sigmoid(x)\n",
        "    if funct==\"relu\":\n",
        "      return relu(x)\n",
        "    if funct==\"tanh\":\n",
        "      return tanh(x)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_loss(X,y,params,lamda):\n",
        "  cnt,loss=0,0\n",
        "  for i in range(len(X)):\n",
        "    predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,params,hidden_layer_size,k,X[i],function)\n",
        "    loss+=cross_entropy(one_hot(y[i]),predicted_y)\n",
        "    if(np.argmax(predicted_y)==y[i]):\n",
        "      cnt+=1\n",
        "  loss+=l2loss(params,lamda)\n",
        "  print('Validation loss:', loss/len(X))\n",
        "  print('Validation accuracy:',cnt/len(X)*100)"
      ],
      "metadata": {
        "id": "v_bb4sYpoJLs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sb3aVZrFnltj"
      },
      "outputs": [],
      "source": [
        "def init(n_in, n_out,initialization):\n",
        "    if(initialization=='normal'):\n",
        "      return np.random.default_rng().uniform(low=-0.69,high=0.69,size=(n_in,n_out))\n",
        "    if(initialization=='xavier'):\n",
        "      variance = 2.0 / (n_in + n_out)\n",
        "      std_dev = np.sqrt(variance)\n",
        "      weights = np.random.rand(n_in, n_out)\n",
        "      return weights/std_dev\n",
        "    if(initialization=='zero'):\n",
        "      return np.zeros((n_in,n_out))\n",
        "\n",
        "\n",
        "def initialize_parameters(number_of_neurons,number_hidden_layers,k,layers,initialization):\n",
        "  parameters={}\n",
        "  parameters['W'+str(1)]=init(layers[0],image_size,initialization)\n",
        "  parameters['b'+str(1)]=init(layers[0],1,initialization)\n",
        "  for i in range(1,number_hidden_layers):\n",
        "    parameters['W'+str(i+1)]=init(layers[i],layers[i-1],initialization)\n",
        "    parameters['b'+str(i+1)]=init(layers[i],1,initialization)\n",
        "  parameters['W'+str(number_hidden_layers+1)]=init(k,layers[-1],initialization)\n",
        "  parameters['b'+str(number_hidden_layers+1)]=init(k,1,initialization)\n",
        "  return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OD0le4R0Rxad"
      },
      "outputs": [],
      "source": [
        "def feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,data,function):\n",
        "  activation={}\n",
        "  pre_activation={}\n",
        "\n",
        "  activation['h0']=data.reshape(784,1)\n",
        "  for i in range(1,number_hidden_layers+1):\n",
        "    a=np.add(parameters['b'+str(i)],np.matmul(parameters['W'+str(i)],activation['h'+str(i-1)]))\n",
        "    h=activation_function(function,a)\n",
        "    #print(h.shape)\n",
        "    #h=sigmoid(a)\n",
        "    pre_activation['a'+str(i)]=a\n",
        "    activation['h'+str(i)]=h\n",
        "  \n",
        "  a=np.add(parameters['b'+str(number_hidden_layers+1)],np.matmul(parameters['W'+str(number_hidden_layers+1)],activation['h'+str(number_hidden_layers)]))\n",
        "  h=softmax(a)\n",
        "  pre_activation['a'+str(number_hidden_layers+1)]=a\n",
        "  activation['h'+str(number_hidden_layers+1)]=h\n",
        "  \n",
        "  return h,activation,pre_activation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gcYvTmD09HAY"
      },
      "outputs": [],
      "source": [
        "#h:: activation a=:preactivation\n",
        "def back_propogation(parameters,activation,pre_activation,X,y,number_hidden_layers,predicted_y,k,lamda,function):\n",
        "\n",
        "  gradient_parameters,gradient_activation,gradient_preactivation={},{},{}\n",
        "\n",
        "  #compute output gradient\n",
        "  e_y=np.zeros((k,1))\n",
        "  e_y[y][0]=1\n",
        "  gradient_preactivation['a'+str(number_hidden_layers+1)]=-(e_y-predicted_y)\n",
        "\n",
        "  for t in range(number_hidden_layers+1,0,-1):\n",
        "    #compute gradients w.r.t parameters\n",
        "    gradient_parameters['W'+str(t)]=np.matmul(gradient_preactivation['a'+str(t)],activation['h'+str(t-1)].T) \n",
        "    gradient_parameters['b'+str(t)]=gradient_preactivation['a'+str(t)]\n",
        "    #print(t,gradient_parameters['W'+str(t)])\n",
        "    if(t==1):break\n",
        "    #compute gradients w.r.t layers below\n",
        "    gradient_activation['h'+str(t-1)]=np.matmul(parameters['W'+str(t)].T,gradient_preactivation['a'+str(t)])\n",
        "\n",
        "    #compute gradients w.r.t preactivation layer\n",
        "    gradient_preactivation['a'+str(t-1)]=np.multiply(gradient_activation['h'+str(t-1)],activation_function(function,pre_activation['a'+str(t-1)],True))\n",
        "  #print(pre_activation)\n",
        "  return gradient_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JiUENjXlKNZN"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters,gradient_change,learning_rate):\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    parameters['W'+str(i)]=parameters['W'+str(i)]-learning_rate*gradient_change['W'+str(i)]\n",
        "    parameters['b'+str(i)]=parameters['b'+str(i)]-learning_rate*gradient_change['b'+str(i)]\n",
        "  return parameters\n",
        "\n",
        "def update_parameters_momentum(parameters,gradient_change,prior_updates,learning_rate,beta):\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    prior_updates['W'+str(i)]=beta*prior_updates['W'+str(i)]+gradient_change['W'+str(i)]\n",
        "    parameters['W'+str(i)]=parameters['W'+str(i)]-learning_rate*prior_updates['W'+str(i)]\n",
        "\n",
        "    prior_updates['b'+str(i)]=beta*prior_updates['b'+str(i)]+gradient_change['b'+str(i)]\n",
        "    parameters['b'+str(i)]=parameters['b'+str(i)]-learning_rate*prior_updates['b'+str(i)]\n",
        "  return parameters,prior_updates\n",
        "\n",
        "def update_parameters_rmsprop(parameters,gradient_change,prior_updates,learning_rate,beta):\n",
        "  epsilon=1e-9\n",
        "  for i in range(1,len(parameters)//2+1):\n",
        "    prior_updates['W'+str(i)]=beta*prior_updates['W'+str(i)]+(1-beta)*(gradient_change['W'+str(i)])**2\n",
        "    parameters['W'+str(i)]=parameters['W'+str(i)]-gradient_change['W'+str(i)]*(learning_rate/np.sqrt(prior_updates['W'+str(i)]+epsilon))\n",
        "\n",
        "    prior_updates['b'+str(i)]=beta*prior_updates['b'+str(i)]+(1-beta)*(gradient_change['b'+str(i)])**2\n",
        "    parameters['b'+str(i)]=parameters['b'+str(i)]-gradient_change['b'+str(i)]*(learning_rate/np.sqrt(prior_updates['b'+str(i)]+epsilon))\n",
        "  return parameters,prior_updates\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DhCxKpquqDHd"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_nag(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function):\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  prior_updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  temp=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  for epoch in range(max_epochs):\n",
        "    loss,cnt=0,0\n",
        "    for it in range(1,len(parameters)//2+1):\n",
        "      updates['W'+str(it)]=prior_updates['W'+str(it)]*beta\n",
        "      updates['b'+str(it)]=prior_updates['b'+str(it)]*beta\n",
        "    \n",
        "    for it in range(1,len(parameters)//2+1):\n",
        "      temp['W'+str(it)]=parameters['W'+str(it)]-updates['W'+str(it)]\n",
        "      temp['b'+str(it)]=parameters['b'+str(it)]-updates['b'+str(it)]\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,temp,hidden_layer_size,k,X[i],function)\n",
        "      loss+=cross_entropy(one_hot(train_labels[i]),predicted_y)\n",
        "      gradient_parameters=back_propogation(temp,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function)\n",
        "      \n",
        "      if(cnt==0):\n",
        "        gradient_change=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "\n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "\n",
        "        #update rule\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          updates['W'+str(it)]=beta*prior_updates['W'+str(it)]+learning_rate*gradient_change['W'+str(it)]\n",
        "          parameters['W'+str(it)]=parameters['W'+str(it)]-updates['W'+str(it)]\n",
        "          temp['W'+str(it)]=parameters['W'+str(it)]\n",
        "          prior_updates['W'+str(it)]=updates['W'+str(it)]\n",
        "\n",
        "          updates['b'+str(it)]=beta*prior_updates['b'+str(it)]+learning_rate*gradient_change['b'+str(it)]\n",
        "          parameters['b'+str(it)]=parameters['b'+str(it)]-updates['b'+str(it)]\n",
        "          temp['b'+str(it)]=parameters['b'+str(it)]\n",
        "          prior_updates['b'+str(it)]=updates['b'+str(it)]\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "    print(epoch,loss/len(X))\n",
        "    validation_loss(val_images_X,val_labels,parameters,lamda)\n",
        "    \n",
        "  return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent_sgd(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function):\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      loss+=cross_entropy(one_hot(train_labels[i]),predicted_y)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function)\n",
        "    \n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "\n",
        "        if(lamda!=0):\n",
        "           for it in range(1,len(parameters)//2+1):\n",
        "             gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "        parameters=update_parameters(parameters,gradient_change,learning_rate)\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "    print(epoch,loss/len(X))\n",
        "    validation_loss(val_images_X,val_labels,parameters,lamda)\n",
        "  return parameters\n"
      ],
      "metadata": {
        "id": "UEuMQiwAjOUw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fh348aaImAeB"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_momentum(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function):\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      #print(activation['h4'])\n",
        "      \n",
        "      loss+=cross_entropy(one_hot(train_labels[i]),predicted_y)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function)\n",
        "      if( epoch==0 and i==0):\n",
        "        #initialize with zero\n",
        "        prior_updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "\n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "\n",
        "        \n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "        parameters,prior_updates=update_parameters_momentum(parameters,gradient_change,prior_updates,learning_rate,beta)\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "    print(epoch,loss/len(X))\n",
        "    validation_loss(val_images_X,val_labels,parameters,lamda)\n",
        "    \n",
        "  return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent_rmsprop(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function):\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      #print(activation['h4'])\n",
        "      \n",
        "      loss+=cross_entropy(one_hot(train_labels[i]),predicted_y)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function)\n",
        "      if( epoch==0 and i==0):\n",
        "        #initialize with zero\n",
        "        prior_updates=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "\n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "\n",
        "        \n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        cnt=0\n",
        "        parameters,prior_updates=update_parameters_rmsprop(parameters,gradient_change,prior_updates,learning_rate,beta)\n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "    print(epoch,loss/len(X))\n",
        "    validation_loss(val_images_X,val_labels,parameters,lamda)\n",
        "  return parameters\n"
      ],
      "metadata": {
        "id": "0RJThIwHzaNu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TFM7Jy9wWckQ"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_adam(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,X,y,k,optimizer,learning_rate,beta,layers,initialization,lamda,function):\n",
        "  beta1=0.9\n",
        "  beta2=0.99\n",
        "  parameters=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,initialization)\n",
        "  momentum=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  momentum_hat=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  update=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "  update_hat=initialize_parameters(hidden_layer_size,number_hidden_layers,k,layers,'zero')\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    loss=0\n",
        "    cnt=0\n",
        "    epsilon=1e-10\n",
        "    for i in range(len(X)):\n",
        "      predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,parameters,hidden_layer_size,k,X[i],function)\n",
        "      loss+=cross_entropy(one_hot(train_labels[i]),predicted_y)\n",
        "      gradient_parameters=back_propogation(parameters,activation,pre_activation,X[i],y[i],number_hidden_layers,predicted_y,k,lamda,function)\n",
        "      if(cnt==0):\n",
        "        gradient_change={}\n",
        "        gradient_change=gradient_parameters.copy()\n",
        "\n",
        "      else:\n",
        "        for iter in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(iter)]+=gradient_parameters['W'+str(iter)]\n",
        "          gradient_change['b'+str(iter)]+=gradient_parameters['b'+str(iter)]\n",
        "      cnt+=1\n",
        "      \n",
        "      if(cnt%batch_size==0 or i==len(X)-1):\n",
        "\n",
        "        if(lamda!=0):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            gradient_change['W'+str(it)]+=np.dot(lamda,parameters['W'+str(it)])\n",
        "\n",
        "        t=cnt if (i==len(X)-1) else batch_size\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          gradient_change['W'+str(it)]=gradient_change['W'+str(it)]/t\n",
        "          gradient_change['b'+str(it)]=gradient_change['b'+str(it)]/t\n",
        "        \n",
        "        cnt=0\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          momentum['W'+str(it)]=beta1*momentum['W'+str(it)]+(1-beta1)*gradient_change['W'+str(it)]\n",
        "          momentum['b'+str(it)]=beta1*momentum['b'+str(it)]+(1-beta1)*gradient_change['b'+str(it)]\n",
        "          momentum_hat['W'+str(it)]=momentum['W'+str(it)]/(1-beta1**(epoch+1))\n",
        "          momentum_hat['b'+str(it)]=momentum['b'+str(it)]/(1-beta1**(epoch+1))\n",
        "\n",
        "        for it in range(1,len(parameters)//2+1):\n",
        "          update['W'+str(it)]=beta2*update['W'+str(it)]+(1-beta2)*gradient_change['W'+str(it)]**2\n",
        "          update['b'+str(it)]=beta2*update['b'+str(it)]+(1-beta2)*gradient_change['b'+str(it)]**2\n",
        "          update_hat['W'+str(it)]=update['W'+str(it)]/(1-beta2**(epoch+1))\n",
        "          update_hat['b'+str(it)]=update['b'+str(it)]/(1-beta2**(epoch+1))\n",
        "\n",
        "        if(optimizer=='adam'):\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            parameters['W'+str(it)]=parameters['W'+str(it)]-(learning_rate*momentum_hat['W'+str(it)]/np.sqrt(update_hat['W'+str(it)]+epsilon))\n",
        "            parameters['b'+str(it)]=parameters['b'+str(it)]-(learning_rate*momentum_hat['b'+str(it)]/np.sqrt(update_hat['b'+str(it)]+epsilon))\n",
        "        else:\n",
        "          for it in range(1,len(parameters)//2+1):\n",
        "            parameters['W'+str(it)]=parameters['W'+str(it)]-(learning_rate/np.sqrt(update_hat['W'+str(it)]+epsilon))*(beta1*momentum_hat['W'+str(it)]+(1-beta1)*gradient_change['W'+str(it)]/(1-beta1**(epoch+1)))\n",
        "            parameters['b'+str(it)]=parameters['b'+str(it)]-(learning_rate/np.sqrt(update_hat['b'+str(it)]+epsilon))*(beta1*momentum_hat['b'+str(it)]+(1-beta1)*gradient_change['b'+str(it)]/(1-beta1**(epoch+1)))\n",
        "    \n",
        "    l2regularizedloss=l2loss(parameters,lamda)\n",
        "    loss+=l2regularizedloss\n",
        "    print(epoch,loss/len(X))\n",
        "    validation_loss(val_images_X,val_labels,parameters,lamda)\n",
        "   \n",
        "  return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IzMicXj7O703"
      },
      "outputs": [],
      "source": [
        "layers=[32,32,32,64]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrHjzVXPMqZo"
      },
      "outputs": [],
      "source": [
        "number_hidden_layers=4\n",
        "hidden_layer_size=32\n",
        "batch_size=32\n",
        "max_epochs=3\n",
        "k=10\n",
        "optimizer='nag'\n",
        "function='tanh'\n",
        "learning_rate=0.01\n",
        "beta=0.9\n",
        "lamda=0.5\n",
        "initialization='normal'\n",
        "params=gradient_descent_nag(number_hidden_layers,hidden_layer_size,batch_size,max_epochs,train_images_X,train_labels,k,optimizer,learning_rate,beta,layers,initialization,lamda,function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AG4r7KxAiO_r"
      },
      "outputs": [],
      "source": [
        "def accuracy(X,y,params):\n",
        "  cnt=0\n",
        "  for i in range(len(X)):\n",
        "    predicted_y,activation,pre_activation=feed_forward(number_hidden_layers,params,hidden_layer_size,k,X[i],function)\n",
        "    if(np.argmax(predicted_y)==y[i]):\n",
        "      cnt+=1\n",
        "  print(cnt/len(X)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIp_tS8NmA2V"
      },
      "outputs": [],
      "source": [
        "accuracy(train_images_X,train_labels,params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0iZ_1mhZxR9",
        "outputId": "3532e698-37ec-4672-ff2c-9cda4cf888ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78.86\n"
          ]
        }
      ],
      "source": [
        "accuracy(test_images_X,test_labels,params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VoHBLyXc9SX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}